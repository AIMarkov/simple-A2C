{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import keras\n",
    "import random\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_greyscale(img): # later on, let's turn these binary\n",
    "    return np.mean(img, axis=2).astype(np.uint8)\n",
    "\n",
    "def rgb_to_gray(rgb):\n",
    "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "\n",
    "    return gray.astype(np.uint8)\n",
    "\n",
    "def downsample(img):\n",
    "    return img[::2, ::2]\n",
    "\n",
    "def preprocess(img):\n",
    "    return rgb_to_gray(downsample(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume a theano backend here, so the \"channels\" are first.\n",
    "#ATARI_SHAPE = (4, 105, 80)\n",
    "\n",
    "# tf backend\n",
    "ATARI_SHAPE = (105, 80, 2)\n",
    "\n",
    "# With the functional API we need to define the inputs.\n",
    "frames_input = keras.layers.Input(ATARI_SHAPE, name='frames')\n",
    "#actions_input = keras.layers.Input((n_actions,), name='mask')\n",
    "\n",
    "# Assuming that the input frames are still encoded from 0 to 255. Transforming to [0, 1].\n",
    "normalized = keras.layers.Lambda(lambda x: x / 255.0)(frames_input)\n",
    "\n",
    "#still want to normalize these, although greyscaled already\n",
    "\n",
    "# \"The first hidden layer convolves 16 8×8 filters with stride 4 with the input image and applies a rectifier nonlinearity.\"\n",
    "conv_1 = keras.layers.convolutional.Convolution2D(\n",
    "    16, 8, 8, subsample=(4, 4), activation='relu'\n",
    ")(normalized)\n",
    "# \"The second hidden layer convolves 32 4×4 filters with stride 2, again followed by a rectifier nonlinearity.\"\n",
    "conv_2 = keras.layers.convolutional.Convolution2D(\n",
    "    32, 4, 4, subsample=(2, 2), activation='relu'\n",
    ")(conv_1)\n",
    "# Flattening the second convolutional layer.\n",
    "conv_flattened = keras.layers.core.Flatten()(conv_2)\n",
    "\n",
    "# \"The final hidden layer is fully-connected and consists of 256 rectifier units.\"\n",
    "hidden = keras.layers.Dense(256, activation='relu')(conv_flattened)\n",
    "\n",
    "# \"The output layer is a fully-connected linear layer with a single output for each valid action.\"\n",
    "output = keras.layers.Dense(4)(hidden) # number of actions\n",
    "\n",
    "#model = keras.models.Model(input=[frames_input, actions_input], output=filtered_output)\n",
    "atari_model = keras.models.Model(input=frames_input, output=output)\n",
    "optimizer = optimizer=keras.optimizers.RMSprop(lr=0.00025, rho=0.95, epsilon=0.01)\n",
    "atari_model.compile(optimizer, loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BreakoutDeterministic-v4\")\n",
    "\n",
    "gamma = 0.9\n",
    "buffer = 10\n",
    "batchSize = 5\n",
    "\n",
    "rewards = []\n",
    "\n",
    "replay = []\n",
    "h = 0\n",
    "\n",
    "model = atari_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02665349,  0.05500542,  0.00534342,  0.06971099]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f37d003d208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frame_old = env.reset()\n",
    "frame_new, reward, is_done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "state = np.array([preprocess(frame_old), preprocess(frame_new)]).reshape(105, 80, 2)\n",
    "\n",
    "plt.imshow(preprocess(frame_old), cmap=\"gray\")\n",
    "\n",
    "model.predict(state.reshape(1,105, 80, 2), batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(1):\n",
    "    \n",
    "# doing first two steps here to get enough for initial state\n",
    "frame_old = env.reset()\n",
    "\n",
    "frame_new, reward, is_done, _ = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old qvals [[ 0.02673113  0.05583856  0.00412825  0.07209937]]\n",
      "action 2\n",
      "reward:  0.0\n",
      "new q vals  [[ 0.03031209  0.0516838   0.00247532  0.06591244]]\n",
      "update 0.0593211963773\n",
      "fitting with the following y: [[ 0.02673113  0.05583856  0.0593212   0.07209937]] \n",
      "\n",
      "\n",
      "old qvals [[ 0.02254781  0.05856476  0.00585566  0.07378579]]\n",
      "action 3\n",
      "reward:  0.0\n",
      "new q vals  [[ 0.02314461  0.05553666  0.00415644  0.07224889]]\n",
      "update 0.0650240018964\n",
      "fitting with the following y: [[ 0.02254781  0.05856476  0.00585566  0.065024  ]] \n",
      "\n",
      "\n",
      "old qvals [[ 0.0283627   0.05491274  0.00202576  0.06808522]]\n",
      "action 0\n",
      "reward:  0.0\n",
      "new q vals  [[ 0.02943253  0.05336015  0.0012679   0.06560936]]\n",
      "update 0.059048422426\n",
      "fitting with the following y: [[ 0.05904842  0.05491274  0.00202576  0.06808522]] \n",
      "\n",
      "\n",
      "old qvals [[ 0.02668936  0.05537083  0.00724179  0.0697626 ]]\n",
      "action 3\n",
      "reward:  0.0\n",
      "new q vals  [[ 0.0212977   0.05906335  0.00916776  0.07266382]]\n",
      "update 0.065397439152\n",
      "fitting with the following y: [[ 0.02668936  0.05537083  0.00724179  0.06539744]] \n",
      "\n",
      "\n",
      "old qvals [[ 0.02079879  0.05612491  0.00295294  0.06989734]]\n",
      "action 1\n",
      "reward:  0.0\n",
      "new q vals  [[ 0.02552857  0.05518865  0.00463092  0.07334452]]\n",
      "update 0.066010069102\n",
      "fitting with the following y: [[ 0.02079879  0.06601007  0.00295294  0.06989734]] \n",
      "\n",
      "\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s - loss: 2.0907e-04\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOUmxoz/zAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f37c8134a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#while not is_done:\n",
    "# STATE\n",
    "orig_state = np.array([preprocess(frame_old), preprocess(frame_new)]).reshape(105, 80, 2)\n",
    "\n",
    "# ACTION\n",
    "Qvals = model.predict(orig_state.reshape(1,105,80,2), batch_size=1)\n",
    "maxQ_ix = np.argmax(Qvals)\n",
    "\n",
    "frame_old = frame_new\n",
    "\n",
    "action = env.action_space.sample()\n",
    "#action = maxQ_ix\n",
    "\n",
    "# NEW STATE, REWARD\n",
    "frame_new, reward, is_done, _ = env.step(action) # totally exploratory\n",
    "\n",
    "# new state \n",
    "new_state = np.array([preprocess(frame_old), preprocess(frame_new)]).reshape(105, 80, 2)\n",
    "\n",
    "reward = np.sign(reward)\n",
    "\n",
    "#Experience replay storage\n",
    "if (len(replay) < buffer): #if buffer not filled, add to it\n",
    "    replay.append((orig_state, action, reward, new_state))\n",
    "else: \n",
    "    if (h < (buffer-1)):\n",
    "        h += 1\n",
    "    else:\n",
    "        h = 0\n",
    "\n",
    "    replay[h] = (orig_state, action, reward, new_state)\n",
    "\n",
    "    #randomly sample our experience replay memory\n",
    "    minibatch = random.sample(replay, batchSize)\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for memory in minibatch:\n",
    "        #Get max_Q(S',a)\n",
    "        old_state, action, reward, new_state = memory\n",
    "        old_qval = model.predict(old_state.reshape(1,105,80,2), batch_size=1)\n",
    "\n",
    "        newQ = model.predict(new_state.reshape(1,105,80,2), batch_size=1)\n",
    "        maxQ = np.max(newQ)\n",
    "        y = np.zeros((1,4))\n",
    "        y[:] = old_qval[:]\n",
    "        update = (reward + (gamma * maxQ))\n",
    "\n",
    "        y[0][action] = update\n",
    "        X_train.append(old_state)\n",
    "        y_train.append(y)\n",
    "        \n",
    "        print(\"old qvals\", old_qval)\n",
    "        print(\"action\", action)\n",
    "        print(\"reward: \", reward)\n",
    "        print(\"new q vals \", newQ)\n",
    "        print(\"update\", update)\n",
    "        print(\"fitting with the following y:\", y, \"\\n\\n\")\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train).reshape(batchSize, 4)\n",
    "\n",
    "    #print(\"Game #: %s\" % (i,))\n",
    "    model.fit(X_train, y_train, batch_size=batchSize, epochs=1, verbose=1)\n",
    "\n",
    "env.render()\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "im = ax.imshow(preprocess(frame_old), cmap=\"gray\") \n",
    "#env.close()\n",
    "\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 105, 80, 2)"
      ]
     },
     "execution_count": 706,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beans/.local/lib/python3.5/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (8, 8), strides=(4, 4), activation=\"relu\")`\n",
      "/home/beans/.local/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (4, 4), strides=(2, 2), activation=\"relu\")`\n",
      "/home/beans/.local/lib/python3.5/site-packages/ipykernel_launcher.py:34: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"fr..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "# We assume a theano backend here, so the \"channels\" are first.\n",
    "#ATARI_SHAPE = (4, 105, 80)\n",
    "\n",
    "# tf backend\n",
    "ATARI_SHAPE = (105, 80, 2)\n",
    "\n",
    "# With the functional API we need to define the inputs.\n",
    "frames_input = keras.layers.Input(ATARI_SHAPE, name='frames')\n",
    "#actions_input = keras.layers.Input((n_actions,), name='mask')\n",
    "\n",
    "# Assuming that the input frames are still encoded from 0 to 255. Transforming to [0, 1].\n",
    "normalized = keras.layers.Lambda(lambda x: x / 255.0)(frames_input)\n",
    "\n",
    "#still want to normalize these, although greyscaled already\n",
    "\n",
    "# \"The first hidden layer convolves 16 8×8 filters with stride 4 with the input image and applies a rectifier nonlinearity.\"\n",
    "conv_1 = keras.layers.convolutional.Convolution2D(\n",
    "    16, 8, 8, subsample=(4, 4), activation='relu'\n",
    ")(normalized)\n",
    "# \"The second hidden layer convolves 32 4×4 filters with stride 2, again followed by a rectifier nonlinearity.\"\n",
    "conv_2 = keras.layers.convolutional.Convolution2D(\n",
    "    32, 4, 4, subsample=(2, 2), activation='relu'\n",
    ")(conv_1)\n",
    "# Flattening the second convolutional layer.\n",
    "conv_flattened = keras.layers.core.Flatten()(conv_2)\n",
    "\n",
    "# \"The final hidden layer is fully-connected and consists of 256 rectifier units.\"\n",
    "hidden = keras.layers.Dense(256, activation='relu')(conv_flattened)\n",
    "\n",
    "# \"The output layer is a fully-connected linear layer with a single output for each valid action.\"\n",
    "output = keras.layers.Dense(4)(hidden) # number of actions\n",
    "\n",
    "#model = keras.models.Model(input=[frames_input, actions_input], output=filtered_output)\n",
    "atari_model = keras.models.Model(input=frames_input, output=output)\n",
    "optimizer = optimizer=keras.optimizers.RMSprop(lr=0.00025, rho=0.95, epsilon=0.01)\n",
    "atari_model.compile(optimizer, loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
